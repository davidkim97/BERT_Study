# Chapter 01. BERT 시작하기

### 1. 트랜스포머 입문
### 2. BERT 이해하기
### 3. BERT 활용하기

---

### 1. 트랜스포머 입문

- 트랜스포머(Transformer) : 자연어 처리에서 주로 사용되는 딥러닝 아키텍처 둥 하나.

- RNN, LSTM 네트워크는 다음 단어 예측, 기계번역, 텍스트 생성 등의 순차적 태스크에서 널리 사용됨.

- 하지만 네트워크는 장기 의존성 문제(long-term dependency)가 있음.

- 이런 RNN의 한계점을 극복하기 위해 트랜스포머라는 아키텍처를 제안.

- 트랜스포머는 RNN에서 사용한 순환 방식을 사용하지 않고 순수하게 어텐션만 사용한 모델.

- 트랜스포머는 셀프 어텐션(self-attention)이라는 특수한 형태의 어텐션을 사용.

- 트랜스포머는 인코더-디코더로 구성된 모델.

- 인코더에 입력 문장(원문)을 입력하면 인코더는 입력 문장의 표현 방법을 학심 시키고 결과를 디코더로 보냄.

- 디코더는 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성.

    ![인코더 디코더](/image/encoder,decoder.png "인코더 디코더")

### 1.1 트랜스포머의 인코더 이해

- 트랜스포머는 N개의 인코더가 쌓인 형태.

    ![인코더 이해](/image/encoder.png "인코더 이해")

- 가장 마지막에 있는 인코더의 결과값이 최종 표현 결과.

- 최초 인코더에 대한 입력값으로 입력 문장을 넣게 되고, 최종 인코더의 결과값으로 입력 문장에 따르는 표현 결과를 얻음.

- 인코더 블록은 두 가지 요소로 구성됨.

    - 멀티 헤드 어텐션(mutil-head attention)

    - 피드포워드 네트워크(feedforward network)

### 1.1.1 셀프 어텐션의 작동 원리

- "A cat ate the fish because it was hungry" 라는 문장이 있다고 가정.

- 문장에서 'it'은 'cat'을 가리키는 것을 알 수 있음.

- 문장에 셀프 어텐션을 적용하면, 모델은 'it'이 'fish'보다 'cat'과 연관성이 큰 것을 알 수 있음.

- 이유는 문장이 모델이 입력되었을 때, 모델이 'it'이라는 단어의 의미를 이해하기 위해 문장 안에 있는 모든 단어와 'it' 이라는 단어의 관계를 계산하는 작업을 수행하기 때문.

    ![셀프 어텐션](/image/self-attention.PNG "셀프 어텐션")

- 문장을 셀프 어텐션에서 사용하려면 임베딩 작업이 필요.

- 셀프 어텐션은 임베딩 된 벡터로부터 쿼리(Q) 행렬, 키(K), 벨류(V) 행렬을 생성하고 이용해서 작동.

    ![셀프 어텐션 예제](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%98%EC%98%88%EC%A0%9C.PNG)

- 위 행렬을 X라고 지칭.
    - 행렬 X에서 첫 번째 'I'의 임베딩, 두 번째 행은 'am'의 임베딩, 세 번째 행은 'good'의 임베딩 의미.

    - X의 차원은 [문장 길이 X 임베딩 차원]의 형태.

- 이렇게 임베딩 된 행렬로부터 세 개의 가중치 행렬을 행렬 X에 곱하면 쿼리 행렬, 키 행렬, 벨류 행렬이 계산됨.

- 세 개의 가중치 행렬은 처음에 임의의 값을 가지며, 학습 과정에서 최적의 값을 얻는다.

    ![셀프 어텐션 예제](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%98%EC%98%88%EC%A0%9C2.PNG)

- 셀프 어텐션의 작동원리
    
    - 셀프 어텐션은 총 4단계로 이루어져 있음.

    - 1단계 : **쿼리(Q) 행렬과 키(K) 행렬의 내적 연산을 수행.**
    
        ![셀프 어텐션 1단계](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%981%EB%8B%A8%EA%B3%84.PNG "셀프어텐션 1단계")

        
        - Q와 K를 내적한 행렬(QK행렬)의 첫 번째 행은 쿼리 벡터1과 키 벡터 1, 2, 3의 내적을 계싼한다는 것을 알 수 있음.

        - 두 벡터 사이의 내적을 계산하면 두 벡터가 얼마나 유사한지 알 수 있음.

        - 즉, I의 쿼리 벡터와 I의 키 벡터, am의 키 벡터, good의 키 벡터 사이의 유사도를 계산한 것.

        - QK행렬의 첫 번째 행은 110, 90, 80이다. 이를 통해 단어 I는 단어 am과 good보다 자신(I)과 연관성이 더 높은 것을 알 수 있음.  
        내적값이 110으로 첫번째 행에서 가장 높기 때문.

     - 2 단계 : **QK행렬의 키 벡터 차원의 제곱근값으로 나눈다. 이렇게 하면 안정적인 경사값을 얻을 수 있음.**

        - 키 벡터의 차원을 64라고 가정하면 이것의 제곱근인 8로 QK행렬을 나눔.

            ![셀프 어텐션 2단계](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%982%EB%8B%A8%EA%B3%84.PNG "셀프어텐션 2단계")

    - 3 단계 : 이전 단계에서 계산한 유사도 값은 비정규화된 형태이기 때문에 소프트맥스 함수를 적용해 정규화 작업 진행

        ![셀프 어텐션 3단계](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%983%EB%8B%A8%EA%B3%84.PNG "셀프 어텐션 3단계")

    - 4 단계 : 앞에서 구한 행렬들을 이용하여 어텐션 행렬(Z)를 계산.  
    어텐션 행렬은 문장의 각 단어의 벡터값을 갖는다.  
    3단계에서 계산한 행렬에 밸류 행렬(V)를 곱하면 어텐션 행렬(Z)를 구할 수 있음.

        ![셀프 어텐션 4단계](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%984%EB%8B%A8%EA%B3%84.PNG)

        - Z를 구하면 단어 good의 셀프 어텐션 z3도 구할 수 있음.

            ![셀프 어텐션 4단계](/image/%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%984%EB%8B%A8%EA%B3%842.PNG)

            - z3은 밸류 벡터 v(I)의 0.21 비중, 벨류 벡터 v(am)의 0.03비중, 벨류 벡터 v(good)의 0.76 비중의 합이라는 것을 알 수 있음.  
            
            - 단어 good의 셀프 벡터값은 벨류 벡터 v(am)보다 v(I)가 많이 반영된 결과로 볼 수 있음.

            - 모델에서 good이 am이 아닌 I와 관련이 크다는 것을 알 수 있음.

            - 셀프 어텐션 방법을 적용하면 이렇게 단어가 문장 내에 있는 다른 단어와 얼마나 연관성이 있는지를 알 수 있음.

- 단계별 정리

    - 1단계 : 쿼리 행렬과 키 행렬 간의 내적을 계산하고 유사도를 구함.

    - 2단계 : 1단계에서 구한 QK행렬을 키 행렬 차원의 제곱근으로 나눈다.

    - 3단계 : 2단계에서 구한 행렬에 소프트맥스 함수를 적용해 정규화 작업을 진행

    - 4단계 : 마지막으로 벨류 행렬을 곱해 어텐션 행렬 Z를 산출.
            
---

### 1.1.2 멀티 헤드 어텐션의 원리
